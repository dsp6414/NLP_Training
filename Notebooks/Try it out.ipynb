{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try it!\n",
    "\n",
    "Before you start ensure the place the SICK test csv file in the Data/SICK folder.\n",
    "The file can be found in this github page under this same folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1: Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext import data\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from scipy.stats import pearsonr\n",
    "from collections import Counter\n",
    "\n",
    "#Reproducing same results\n",
    "seed = 2020\n",
    "\n",
    "#Set the seed to be fixed\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') \n",
    "\n",
    "#Global\n",
    "TEST_BATCH_SIZE = 13\n",
    "\n",
    "#Load the test set\n",
    "classif_test_label = data.LabelField(dtype=torch.int64, batch_first=True)\n",
    "classif_test_sentAB = data.Field(tokenize='spacy', include_lengths=True, batch_first=True)\n",
    "\n",
    "classif_test_fields = [(None, None), (None, None), (None, None), (None, None), (None, None),\n",
    "                       ('test_label', classif_test_label), ('test_sentAB', classif_test_sentAB)]\n",
    "\n",
    "classif_test_dataset = data.TabularDataset(path='../DATA/SICK/SICK test.csv', format='CSV', \n",
    "                                           fields=classif_test_fields, skip_header=True)\n",
    "\n",
    "classif_test_sentAB.build_vocab(classif_test_dataset, min_freq=1, vectors_cache=\"Vectors/\",\n",
    "                                vectors=\"glove.6B.300d\")\n",
    "\n",
    "classif_test_label.build_vocab(classif_test_dataset)\n",
    "\n",
    "classif_test_iterator = data.BucketIterator(classif_test_dataset, TEST_BATCH_SIZE,\n",
    "                                             sort_key=lambda x : x.classif_test_sentAB,\n",
    "                                             device=device,\n",
    "                                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNClassifModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim,\n",
    "                 output_dim, num_layers, bidirectional,dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #More regularization\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        #LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        #More regularization\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        #Full connected layer\n",
    "        self.fc = nn.Linear(2 * hidden_dim, output_dim)\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [batch size, max sentence length in batch]\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        #Regularize!\n",
    "        reg_embedded = self.dropout1(embedded)\n",
    "        \n",
    "        #pack the batch sentences to max length\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(reg_embedded, text_lengths, \n",
    "                                                            batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        \n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        \n",
    "        inputs = self.dropout2(hidden)\n",
    "        \n",
    "        #Direct values are given to CE loss for loss calculation\n",
    "        #LogSoftmax is used for inference\n",
    "        outputs = self.fc(inputs)\n",
    "    \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNClassifModel(\n",
       "  (embedding): Embedding(2298, 300)\n",
       "  (dropout1): Dropout(p=0.4, inplace=False)\n",
       "  (lstm): LSTM(300, 32, batch_first=True, bidirectional=True)\n",
       "  (dropout2): Dropout(p=0.4, inplace=False)\n",
       "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 2298 #Size of the training set vocabulary\n",
    "embedding_dim = 300\n",
    "num_hidden_nodes = 32\n",
    "num_output_nodes = 3\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "dropout_rate = 0.4\n",
    "\n",
    "#Create the model\n",
    "model = RNNClassifModel(vocab_size, embedding_dim, num_hidden_nodes,\n",
    "                 num_output_nodes, num_layers, bidirectional,dropout_rate)\n",
    "\n",
    "#Load the pretrained model\n",
    "checkpoint = torch.load('../Models/bilstm_task1_fn.pth')\n",
    "model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "\n",
    "#Optimizer and Loss\n",
    "classif_criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "#Softmax layer only used to get probabilties\n",
    "softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_prediction(dataset, iterator, criterion):\n",
    "    iterations = len(dataset) / TEST_BATCH_SIZE\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0\n",
    "    running_correct = 0\n",
    "    _predictions = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, items in enumerate(iterator):\n",
    "  \n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write(\"{} Iteration :{}/{}\"\n",
    "                                .format(\"Prediction\", batch_idx + 1, iterations))\n",
    "\n",
    "\n",
    "            #Get the text and length of sentences\n",
    "            text, text_lengths = items.test_sentAB\n",
    "            text = text.to(device)\n",
    "            text_lengths = text_lengths.to(device)\n",
    "\n",
    "            #Get labels of each batch\n",
    "            labels = items.test_label\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            #Predictions are in size [1, ..]\n",
    "            outputs = model(text, text_lengths).squeeze()\n",
    "\n",
    "            #Calculate loss\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            #Convert predictions to probabilities\n",
    "            probabilites = softmax(outputs)\n",
    "            #Give's the index of the node with the highest probability\n",
    "            predictions = torch.argmax(probabilites, dim=1)\n",
    "            correct = (predictions == labels).float()\n",
    "\n",
    "            #Save the predictions in a list\n",
    "            _predictions += list(predictions.cpu().squeeze().numpy())\n",
    "            \n",
    "            #Calculate running loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "            running_correct += correct.sum().item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(iterator)\n",
    "        epoch_accuracy = running_correct / len(dataset)\n",
    "        \n",
    "    return epoch_loss, epoch_accuracy, _predictions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Iteration :379/379.0"
     ]
    }
   ],
   "source": [
    "classif_loss, classif_accuracy, classif_predictions = classification_prediction(classif_test_dataset, \n",
    "                                                                                classif_test_iterator, \n",
    "                                                                                classif_criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9488168171653647, 0.565049725999594)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Classification results\n",
    "classif_loss, classif_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2: Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the test set\n",
    "regress_test_score = data.Field(dtype=torch.float, use_vocab=False, batch_first=True, \n",
    "                         is_target=True, sequential=False)\n",
    "regress_test_sentAB = data.Field(tokenize='spacy', include_lengths=True, batch_first=True)\n",
    "\n",
    "regress_test_fields = [(None, None), (None, None), (None, None), ('test_score', regress_test_score),\n",
    "                        (None, None), (None, None), ('test_sentAB', regress_test_sentAB)]\n",
    "\n",
    "regress_test_dataset = data.TabularDataset(path='../DATA/SICK/SICK test.csv', format='CSV', \n",
    "                                           fields=regress_test_fields, skip_header=True)\n",
    "\n",
    "regress_test_sentAB.build_vocab(regress_test_dataset, min_freq=1, vectors_cache=\"Vectors/\",\n",
    "                                vectors=\"glove.6B.300d\")\n",
    "\n",
    "regress_test_label.build_vocab(regress_test_dataset)\n",
    "\n",
    "regress_test_iterator = data.BucketIterator(regress_test_dataset, TEST_BATCH_SIZE,\n",
    "                                             sort_key=lambda x : x.test_sentAB,\n",
    "                                             device=device,\n",
    "                                             shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNRegressModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim,\n",
    "                 output_dim, num_layers, bidirectional,dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #More regularization\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        #LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            bidirectional=bidirectional,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        #More regularization\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        #Full connected layer\n",
    "        self.fc = nn.Linear(2 * hidden_dim, output_dim)\n",
    "        \n",
    "        #Activation function\n",
    "        self.act_fn = nn.ReLU()\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [batch size, max sentence length in batch]\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        #Regularize!\n",
    "        reg_embedded = self.dropout1(embedded)\n",
    "        \n",
    "        #pack the batch sentences to max length\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(reg_embedded, text_lengths, \n",
    "                                                            batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        \n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        \n",
    "        inputs = self.dropout2(hidden)\n",
    "        \n",
    "        #Direct values are given to CE loss for loss calculation\n",
    "        outputs = self.fc(inputs)\n",
    "        \n",
    "        outputs = self.act_fn(outputs)\n",
    "    \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNRegressModel(\n",
       "  (embedding): Embedding(2298, 300)\n",
       "  (dropout1): Dropout(p=0.4, inplace=False)\n",
       "  (lstm): LSTM(300, 32, batch_first=True, bidirectional=True)\n",
       "  (dropout2): Dropout(p=0.4, inplace=False)\n",
       "  (fc): Linear(in_features=64, out_features=3, bias=True)\n",
       "  (act_fn): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = 2298 #Size of the training set vocabulary\n",
    "embedding_dim = 300\n",
    "num_hidden_nodes = 32\n",
    "num_output_nodes = 1\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "dropout_rate = 0.4\n",
    "\n",
    "#Create the model\n",
    "regress_model = RNNRegressModel(vocab_size, embedding_dim, num_hidden_nodes,\n",
    "                                num_output_nodes, num_layers, bidirectional,dropout_rate)\n",
    "\n",
    "#Load the pretrained model\n",
    "checkpoint = torch.load('../Models/bilstm_task2_2dropout_1e23_1.pth')\n",
    "regress_model.load_state_dict(checkpoint[\"model\"])\n",
    "\n",
    "\n",
    "#Optimizer and Loss\n",
    "regress_criterion = nn.MSELoss()\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_prediction(model, dataset, iterator, criterion):\n",
    "    iterations = len(dataset) / TEST_BATCH_SIZE\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0\n",
    "    running_correct = 0\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, items in enumerate(iterator):\n",
    "            \n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write(\"{} Iteration :{}/{}\"\n",
    "                                .format(\"Prediction\", batch_idx + 1, iterations))\n",
    "\n",
    "\n",
    "            #Get the text and length of sentences\n",
    "            text, text_lengths = items.test_sentAB\n",
    "            text = text.to(device)\n",
    "            text_lengths = text_lengths.to(device)\n",
    "\n",
    "            #Get labels of each batch\n",
    "            scores = items.test_score\n",
    "            scores = scores.to(device)\n",
    "\n",
    "            #Predictions are in size [1, ..]\n",
    "            outputs = model(text, text_lengths).squeeze()\n",
    "            \n",
    "            #Calculate loss\n",
    "            loss = criterion(outputs, scores)\n",
    "            \n",
    "\n",
    "            #Get the prediction outputs and targets into a list for eval function\n",
    "            predictions += list(outputs.detach().cpu().numpy())\n",
    "            targets += list(scores.cpu().numpy())\n",
    "\n",
    "            \n",
    "            #Calculate running loss and accuracy\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        epoch_loss = running_loss / len(iterator)\n",
    "        \n",
    "    return epoch_loss, np.array(predictions), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction Iteration :379/379.0"
     ]
    }
   ],
   "source": [
    "regress_loss, regress_score, regress_predictions = regression_prediction(regress_model,\n",
    "                                                                            regress_test_dataset, \n",
    "                                                                            regress_test_iterator, \n",
    "                                                                            regress_criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1.026610275686259,\n",
       " array([3.3450677, 3.4405046, 3.413593 , ..., 3.3067756, 3.8597636,\n",
       "        3.6444664], dtype=float32))"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "regress_loss, regress_score"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
