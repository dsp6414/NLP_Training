{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchtext import data\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sys\n",
    "from scipy.stats import pearsonr\n",
    "from collections import Counter\n",
    "\n",
    "#Reproducing same results\n",
    "seed = 2020\n",
    "\n",
    "#Set the seed to be fixed\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# How to make a regression training set\n",
    "#Fields for each set of data\n",
    "train_score = data.Field(dtype=torch.float, use_vocab=False, batch_first=True, \n",
    "                         is_target=True, sequential=False)\n",
    "train_sentAB = data.Field(tokenize='spacy', include_lengths=True, batch_first=True)\n",
    "\n",
    "test_score = data.Field(dtype=torch.float, use_vocab=False, batch_first=True, \n",
    "                         is_target=True, sequential=False)\n",
    "test_sentAB = data.Field(tokenize='spacy', include_lengths=True, batch_first=True)\n",
    "\n",
    "train_fields = [(None, None), (None, None), (None, None), ('train_score', train_score),\n",
    "                (None, None), (None, None), ('train_sentAB', train_sentAB)]\n",
    "test_fields = [(None, None), (None, None), (None, None), ('test_score', test_score),\n",
    "               (None, None), (None, None), ('test_sentAB', test_sentAB)]\n",
    "\n",
    "#Complete datasets of train and test\n",
    "train_dataset = data.TabularDataset(path='../DATA/SICK/SICK train val.csv', format='CSV', \n",
    "                                    fields=train_fields, skip_header=True)\n",
    "test_dataset = data.TabularDataset(path='../DATA/SICK/SICK test.csv', format='CSV', \n",
    "                                    fields=test_fields, skip_header=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 8\n",
    "TEST_BATCH_SIZE = 13\n",
    "\n",
    "train_sentAB.build_vocab(train_dataset, min_freq=1, vectors_cache=\"Vectors/\",\n",
    "                         vectors=\"glove.6B.300d\")\n",
    "\n",
    "test_sentAB.build_vocab(test_dataset, min_freq=1, vectors_cache=\"Vectors/\",\n",
    "                        vectors=\"glove.6B.300d\")\n",
    "\n",
    "train_iterator = data.BucketIterator(train_dataset, TRAIN_BATCH_SIZE,\n",
    "                                     device=device,\n",
    "                                     shuffle=True)\n",
    "\n",
    "test_iterator = data.BucketIterator(test_dataset, TEST_BATCH_SIZE,\n",
    "                                     device=device,\n",
    "                                     shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The', 14)\n",
      "('young', 45)\n",
      "('boys', 109)\n",
      "('are', 12)\n",
      "('playing', 18)\n",
      "('outdoors', 174)\n",
      "('and', 11)\n",
      "('the', 8)\n",
      "('man', 9)\n",
      "('is', 2)\n"
     ]
    }
   ],
   "source": [
    "#Example of word is matched to index from embeddings\n",
    "for word in train_dataset.examples[2].train_sentAB[:10]:\n",
    "    print((word, train_sentAB.vocab.stoi[word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2298\n",
      "2273\n"
     ]
    }
   ],
   "source": [
    "print(len(train_sentAB.vocab))\n",
    "\n",
    "print(len(test_sentAB.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim,\n",
    "                 output_dim, num_layers, bidirectional,dropout_rate):\n",
    "        super().__init__()\n",
    "        \n",
    "        #Embedding layer\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        #More regularization\n",
    "#         self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        #LSTM layer\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim,\n",
    "                            num_layers=num_layers,\n",
    "                            bidirectional=bidirectional,\n",
    "#                             dropout=dropout_rate,\n",
    "                            batch_first=True)\n",
    "        \n",
    "        #More regularization\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "        #Full connected layer\n",
    "        self.fc = nn.Linear(2 * hidden_dim, output_dim)\n",
    "        \n",
    "        #Activation function\n",
    "        self.act_fn = nn.ReLU()\n",
    "        \n",
    "    def forward(self, text, text_lengths):\n",
    "        \n",
    "        #text = [batch size, max sentence length in batch]\n",
    "        embedded = self.embedding(text)\n",
    "        \n",
    "        #Regularize!\n",
    "#         reg_embedded = self.dropout1(embedded)\n",
    "        \n",
    "        #pack the batch sentences to max length\n",
    "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths, \n",
    "                                                            batch_first=True, enforce_sorted=False)\n",
    "        \n",
    "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
    "        \n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        \n",
    "        inputs = self.dropout2(hidden)\n",
    "        \n",
    "        #Direct values are given to CE loss for loss calculation\n",
    "        #LogSoftmax is used for inference\n",
    "        outputs = self.fc(inputs)\n",
    "        \n",
    "        outputs = self.act_fn(outputs)\n",
    "    \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNNModel(\n",
       "  (embedding): Embedding(2298, 300)\n",
       "  (lstm): LSTM(300, 32, batch_first=True, bidirectional=True)\n",
       "  (dropout2): Dropout(p=0.4, inplace=False)\n",
       "  (fc): Linear(in_features=64, out_features=1, bias=True)\n",
       "  (act_fn): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(train_sentAB.vocab)\n",
    "embedding_dim = 300\n",
    "num_hidden_nodes = 32\n",
    "num_output_nodes = 1\n",
    "num_layers = 1\n",
    "bidirectional = True\n",
    "dropout_rate = 0.4\n",
    "\n",
    "#Create the model\n",
    "model = RNNModel(vocab_size, embedding_dim, num_hidden_nodes,\n",
    "                 num_output_nodes, num_layers, bidirectional,dropout_rate)\n",
    "model.to(device)\n",
    "\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of embedding matrix torch.Size([2298, 300])\n",
      "Number of trainable elements in the model 774969\n"
     ]
    }
   ],
   "source": [
    "#Set pretrained word embedding as weights for embedding layer\n",
    "embeddings = train_sentAB.vocab.vectors\n",
    "model.embedding.weight.data.copy_(embeddings)\n",
    "\n",
    "print(\"Size of embedding matrix {}\".format(embeddings.size()))\n",
    "\n",
    "num_elements = 0\n",
    "for params in model.parameters():\n",
    "    if params.requires_grad:\n",
    "        num_elements += params.numel()\n",
    "    \n",
    "print(\"Number of trainable elements in the model {}\".format(num_elements))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Optimizer and Loss\n",
    "# optimizer = optim.Adam(model.parameters(), lr=1e-2)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# scheduler = optim.lr_scheduler.CyclicLR(optimizer, base_lr=1e-3,\n",
    "#                                         max_lr=1e-2, step_size_up=1250, mode='triangular2')\n",
    "# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=6, gamma=0.1)\n",
    "scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[3])\n",
    "\n",
    "#Softmax layer only used to get probabilties\n",
    "softmax = nn.LogSoftmax(dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Train the model\n",
    "\n",
    "def train(dataset, iterator, optimizer, scheduler, criterion):\n",
    "    iterations = len(dataset) / TRAIN_BATCH_SIZE\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    running_loss = 0\n",
    "    running_correct = 0\n",
    "    phase = \"train\"\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    for batch_idx, items in enumerate(iterator):\n",
    "\n",
    "        sys.stdout.write('\\r')\n",
    "        sys.stdout.write(\"{} Iteration :{}/{}\"\n",
    "                            .format(phase, batch_idx + 1, iterations))\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        #Get the text and length of sentences\n",
    "        text, text_lengths = items.train_sentAB\n",
    "        text = text.to(device)\n",
    "        text_lengths = text_lengths.to(device)\n",
    "\n",
    "        #Get scores of each batch\n",
    "        scores = items.train_score\n",
    "        scores = scores.to(device)\n",
    "\n",
    "        #Predictions are in size [1, ..]\n",
    "        outputs = model(text, text_lengths).squeeze()\n",
    "\n",
    "        #Calculate loss\n",
    "        loss = criterion(outputs, scores)\n",
    "\n",
    "        #Get the prediction outputs and targets into a list for eval function\n",
    "        predictions += list(outputs.detach().cpu().numpy())\n",
    "        targets += list(scores.cpu().numpy())\n",
    "\n",
    "        loss.backward()   \n",
    "        optimizer.step()\n",
    "        \n",
    "        #Learning rate scheduler to improve convergence\n",
    "#         scheduler.step() \n",
    "\n",
    "        #Save the loss details for each epoch\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    epoch_loss = running_loss / len(iterator)\n",
    "    \n",
    "    #Numpy arrays are more memory efficient\n",
    "    return epoch_loss, np.array(predictions), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate the model on the training and test set\n",
    "def evaluate(dataset, iterator, optimizer, criterion):\n",
    "    iterations = len(test_dataset) / TEST_BATCH_SIZE\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    running_loss = 0\n",
    "    running_correct = 0\n",
    "    phase = \"test\"\n",
    "    predictions = []\n",
    "    targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch_idx, items in enumerate(iterator):\n",
    "\n",
    "            sys.stdout.write('\\r')\n",
    "            sys.stdout.write(\"{} Iteration :{}/{}\"\n",
    "                                .format(phase, batch_idx + 1, iterations))\n",
    "\n",
    "            #Get the text and length of sentences\n",
    "            text, text_lengths = items.test_sentAB\n",
    "            text = text.to(device)\n",
    "            text_lengths = text_lengths.to(device)\n",
    "\n",
    "            #Get labels of each batch\n",
    "            scores = items.test_score\n",
    "            scores = scores.to(device)\n",
    "\n",
    "            #Predictions are in size [1, ..]\n",
    "            outputs = model(text, scores).squeeze()\n",
    "\n",
    "            #Calculate loss\n",
    "            loss = criterion(outputs, scores)\n",
    "            \n",
    "            #Get the prediction outputs and targets into a list for eval function\n",
    "            predictions += list(outputs.detach().cpu().numpy())\n",
    "            targets += list(scores.cpu().numpy())\n",
    "        \n",
    "            #Save the loss details for each epoch\n",
    "            running_loss += loss.item()\n",
    "\n",
    "\n",
    "        epoch_loss = running_loss / len(test_iterator)\n",
    "\n",
    "    \n",
    "    return epoch_loss, np.array(predictions), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :1/8\n",
      "0.01\n",
      "train Iteration :625/625.0\n",
      "test Iteration :379/379.0\n",
      "Train Loss :1.198 | PC (Train) :0.011 | Eval Loss :1.027 | PC (Test) :0.022\n",
      "Epoch :2/8\n",
      "0.01\n",
      "train Iteration :625/625.0\n",
      "test Iteration :379/379.0\n",
      "Train Loss :1.047 | PC (Train) :0.112 | Eval Loss :1.038 | PC (Test) :0.036\n",
      "Epoch :3/8\n",
      "0.01\n",
      "train Iteration :625/625.0\n",
      "test Iteration :379/379.0\n",
      "Train Loss :1.014 | PC (Train) :0.191 | Eval Loss :0.993 | PC (Test) :0.196\n",
      "Epoch :4/8\n",
      "0.001\n",
      "train Iteration :625/625.0\n",
      "test Iteration :379/379.0\n",
      "Train Loss :0.954 | PC (Train) :0.299 | Eval Loss :1.005 | PC (Test) :0.123\n",
      "Epoch :5/8\n",
      "0.001\n",
      "train Iteration :625/625.0\n",
      "test Iteration :379/379.0\n",
      "Train Loss :0.946 | PC (Train) :0.312 | Eval Loss :1.004 | PC (Test) :0.129\n",
      "Epoch :6/8\n",
      "0.001\n",
      "train Iteration :625/625.0\n",
      "test Iteration :379/379.0\n",
      "Train Loss :0.938 | PC (Train) :0.323 | Eval Loss :1.010 | PC (Test) :0.113\n",
      "Epoch :7/8\n",
      "0.001\n",
      "train Iteration :625/625.0\n",
      "test Iteration :379/379.0\n",
      "Train Loss :0.928 | PC (Train) :0.338 | Eval Loss :1.014 | PC (Test) :0.104\n",
      "Epoch :8/8\n",
      "0.001\n",
      "train Iteration :625/625.0\n",
      "test Iteration :379/379.0\n",
      "Train Loss :0.924 | PC (Train) :0.343 | Eval Loss :1.016 | PC (Test) :0.105\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 8\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    print(\"Epoch :{}/{}\".format(epoch + 1, num_epochs))\n",
    "    \n",
    "    print(optimizer.param_groups[0][\"lr\"])\n",
    "    \n",
    "    train_loss, train_pred, train_targets = train(train_dataset, train_iterator, \n",
    "                                                  optimizer, scheduler, criterion) \n",
    "    print()\n",
    "    \n",
    "    scheduler.step()\n",
    "    \n",
    "    test_loss, test_pred, test_targets = evaluate(test_dataset, test_iterator, optimizer, criterion) \n",
    "    print()\n",
    "    \n",
    "    \n",
    "    print(\"Train Loss :{:.3f} | PC (Train) :{:.3f} | \"\\\n",
    "          \"Eval Loss :{:.3f} | PC (Test) :{:.3f}\".format(train_loss, \n",
    "                                                         pearsonr(train_pred, train_targets)[0], \n",
    "                                                         test_loss, \n",
    "                                                         pearsonr(test_pred, test_targets)[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the model. Comment immediately after saving\n",
    "torch.save({ \"optimizer\" : optimizer.state_dict(),\n",
    "             \"scheduler\" : scheduler.state_dict(),\n",
    "             \"model\":model.state_dict(),\n",
    "             \"epoch\" : 8}, '../Models/bilstm_task2_fn.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
