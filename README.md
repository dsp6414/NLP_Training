# NLP_Training
NLP examples 

Check the How to run the code.txt file on how to use these notebooks.

We have downloaded the word embeddings and datasets from these links:

Word Embeddings: <br/>
Glove: https://github.com/stanfordnlp/GloVe/blob/master/README.md <br/>
Fast Text: https://fasttext.cc/docs/en/english-vectors.html <br/>
LexVec: https://github.com/alexandres/lexvec <br/>
ConceptNet NumberBatch: https://github.com/commonsense/conceptnet-numberbatch <br/>
Google News: https://code.google.com/archive/p/word2vec/ <br/>
PDC: http://ofey.me/projects/wordrep/ <br/>
HDC: http://ofey.me/projects/wordrep/ <br/>

Similarity datasets:

MTurk : Available on this repo <br/>
MEN : Available on this repo <br/>
WS353 : http://www.cs.technion.ac.il/~gabr/resources/data/wordsim353/ also available on this repo <br/>
RG65 : Available on this repo <br/>
RW : https://nlp.stanford.edu/~lmthang/morphoNLM/ also available on this Available on this repo <br/>
SimLex999 :https://fh295.github.io/simlex.html <br/>
TR9856 : Available on this repo <br/>

Analogy datasets
MSR WordRep : Available on this repo <br/>
Google Analogy : Available on this repo <br/>
MSR : Available on this repo <br/>
SEMEVAL 2012 Task 2 : Available on this repo <br/>


Important references for Text classification with PyTorch
https://www.analyticsvidhya.com/blog/2020/01/first-text-classification-in-pytorch/

https://github.com/pchampio/sentence-entailment


Description of Experiments file:

Model Description
LSTIM(Bidirectional, Number of Layers, Num of Hidden Nodes, Dropout rate)

